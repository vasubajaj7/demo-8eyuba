# Core Airflow Configuration
AIRFLOW_HOME=/opt/airflow
AIRFLOW__CORE__EXECUTOR=CeleryExecutor                  # Task execution method, CeleryExecutor recommended for distributed environments
AIRFLOW__CORE__DAGS_FOLDER=/opt/airflow/dags            # Path to DAG files directory
AIRFLOW__CORE__LOAD_EXAMPLES=False                      # Whether to load example DAGs, should be disabled in production
AIRFLOW__CORE__FERNET_KEY=                              # Key for encrypting sensitive information in the database
AIRFLOW__CORE__PARALLELISM=32                           # Maximum number of task instances that can run concurrently
AIRFLOW__CORE__DAG_CONCURRENCY=16                       # Number of task instances allowed to run concurrently per DAG
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=16               # Maximum number of active DAG runs per DAG
AIRFLOW__CORE__DAGBAG_IMPORT_TIMEOUT=60                 # Timeout in seconds for importing DAGs
AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC                     # Default timezone for Airflow schedules and operations

# Database Configuration
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow    # Connection string for the metadata database
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_SIZE=5              # Size of the SQLAlchemy connection pool
AIRFLOW__DATABASE__SQL_ALCHEMY_POOL_RECYCLE=1800        # Connection recycling time in seconds
AIRFLOW__DATABASE__SQL_ALCHEMY_MAX_OVERFLOW=10          # Maximum overflow size of the SQLAlchemy pool

# Celery Configuration
AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/0      # URL of the Celery broker (Redis in this case)
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow    # URL of the Celery result backend
AIRFLOW__CELERY__WORKER_CONCURRENCY=8                   # Number of tasks a Celery worker can execute at once
AIRFLOW__CELERY__WORKER_PREFETCH_MULTIPLIER=1           # Number of tasks prefetched per worker
AIRFLOW__CELERY__TASK_ACKS_LATE=True                    # Whether to acknowledge tasks after execution instead of before

# Webserver Configuration
AIRFLOW__WEBSERVER__BASE_URL=http://localhost:8080      # Base URL for the web interface
AIRFLOW__WEBSERVER__SECRET_KEY=                         # Secret key for the web interface session
AIRFLOW__WEBSERVER__WORKERS=4                           # Number of Gunicorn workers for the webserver
AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE=1         # Number of workers to refresh at once
AIRFLOW__WEBSERVER__WEB_SERVER_PORT=8080                # Port on which to run the web server
AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX=True               # Enable werkzeug ProxyFix middleware for reverse proxy support

# Authentication and Security
AIRFLOW__API__AUTH_BACKENDS=airflow.api.auth.backend.basic_auth    # Authentication backend for the Airflow API
AIRFLOW__WEBSERVER__RBAC=True                           # Enable Role-Based Access Control
AIRFLOW__WEBSERVER__AUTHENTICATE=True                   # Whether the webserver should use authentication
AIRFLOW__WEBSERVER__AUTH_BACKEND=airflow.providers.google.common.auth_backend.google_oauth    # Authentication backend for the webserver
GOOGLE_OAUTH_CLIENT_ID=                                 # Google OAuth client ID for authentication
GOOGLE_OAUTH_CLIENT_SECRET=                             # Google OAuth client secret for authentication
GOOGLE_OAUTH_AUDIENCE=                                  # Google OAuth audience

# GCP Configuration
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/gcp-service-account-key.json    # Path to Google Cloud service account key file
GCP_PROJECT_ID=composer2-migration-project-dev          # Google Cloud Project ID
GCP_LOCATION=us-central1                                # Google Cloud location (region or zone)
COMPOSER_ENVIRONMENT=composer2-dev                      # Cloud Composer environment name

# Logging Configuration
AIRFLOW__LOGGING__REMOTE_LOGGING=True                   # Whether to enable remote logging
AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER=gs://composer2-migration-dev-logs/logs    # Remote logging path (GCS bucket for Cloud Composer)
AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID=google_cloud_default    # The connection to use for remote logging
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO                    # Logging level for Airflow logs
AIRFLOW__LOGGING__DAG_PROCESSOR_MANAGER_LOG_LOCATION=/opt/airflow/logs/dag_processor_manager/dag_processor_manager.log    # Where to store the DAG processor manager logs

# Email Configuration
AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp    # Email backend to use for sending emails
AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com                 # SMTP server hostname
AIRFLOW__SMTP__SMTP_PORT=587                            # SMTP server port
AIRFLOW__SMTP__SMTP_USER=                               # SMTP username
AIRFLOW__SMTP__SMTP_PASSWORD=                           # SMTP password
AIRFLOW__SMTP__SMTP_MAIL_FROM=airflow@example.com       # Email address used as the sender in Airflow emails

# Storage Configuration
AIRFLOW_VAR_GCS_BUCKET=composer2-migration-dev-bucket   # Main GCS bucket for Airflow storage, exposed as an Airflow variable
AIRFLOW_VAR_DAG_BUCKET=composer2-migration-dev-dags     # GCS bucket for DAG storage, exposed as an Airflow variable
AIRFLOW_VAR_DATA_BUCKET=composer2-migration-dev-data    # GCS bucket for data storage, exposed as an Airflow variable
AIRFLOW_VAR_LOGS_BUCKET=composer2-migration-dev-logs    # GCS bucket for log storage, exposed as an Airflow variable

# Environment-Specific Settings
AIRFLOW_VAR_ENV=dev                                     # Current environment (dev, qa, prod), exposed as an Airflow variable
AIRFLOW_VAR_ENVIRONMENT_NAME=Development                # Human-readable environment name, exposed as an Airflow variable
AIRFLOW_VAR_IS_PRODUCTION=False                         # Flag indicating if this is a production environment, exposed as an Airflow variable
ENVIRONMENT=dev                                         # Current environment name (internal use)
DEBUG=True                                              # Enable debug mode in development environments

# Secret Manager Integration
AIRFLOW_CONN_GOOGLE_CLOUD_DEFAULT=google-cloud-platform://?extra__google_cloud_platform__project=composer2-migration-project-dev&extra__google_cloud_platform__key_path=/opt/airflow/gcp-service-account-key.json    # Default Google Cloud connection
AIRFLOW_CONN_POSTGRES_DEFAULT=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow    # Default PostgreSQL connection
AIRFLOW__SECRETS__BACKEND=airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend    # Backend to use for storing secrets
AIRFLOW__SECRETS__BACKEND_KWARGS={"connections_prefix": "airflow-connections", "variables_prefix": "airflow-variables", "gcp_key_path": "/opt/airflow/gcp-service-account-key.json", "project_id": "composer2-migration-project-dev"}    # Configuration for the secret backend in JSON format

# Pool Configuration
AIRFLOW_POOL_DEFAULT_POOL=64                            # Number of slots for the default pool
AIRFLOW_POOL_ETL_POOL=16                                # Number of slots for the ETL pool
AIRFLOW_POOL_DATA_SYNC_POOL=12                          # Number of slots for the data sync pool
AIRFLOW_POOL_REPORTING_POOL=8                           # Number of slots for the reporting pool

# Monitoring Configuration
AIRFLOW__METRICS__STATSD_ON=True                        # Enable StatsD metrics collection
AIRFLOW__METRICS__STATSD_HOST=statsd-exporter           # StatsD host
AIRFLOW__METRICS__STATSD_PORT=9125                      # StatsD port
AIRFLOW__METRICS__STATSD_PREFIX=airflow                 # Prefix for StatsD metrics
AIRFLOW_VAR_ENABLE_ALERTS=True                          # Enable alerting for DAG failures, exposed as an Airflow variable

# Airflow 2.X Specific Configuration
AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC=10          # Scheduler heartbeat interval in seconds (new in Airflow 2.X)
AIRFLOW__SCHEDULER__PARSING_PROCESSES=2                 # Number of processes to use for DAG file parsing (new in Airflow 2.X)
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=60            # Interval in seconds to scan the DAGs directory for new files
AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR=True       # Run DAG processing in a separate process (new in Airflow 2.X)
AIRFLOW__SCHEDULER__USE_JOB_SCHEDULE=True               # Use job scheduling for periodic tasks (new in Airflow 2.X)