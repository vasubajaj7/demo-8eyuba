"""
Apache Airflow 2.X Configuration Generator for Cloud Composer 2

This script generates the airflow.cfg configuration file for Apache Airflow 2.X 
in Cloud Composer 2, incorporating environment-specific settings from the 
appropriate configuration module (dev, qa, or prod).

The configuration includes all runtime settings required for Airflow, such as:
- Database connections
- Security parameters
- Executor settings
- Webserver configuration
- Environment-specific overrides

This is a critical component of the migration from Airflow 1.10.15 to Airflow 2.X.
"""

import os
import configparser
import socket

# Define global constants
ENV_VAR_PREFIX = 'AIRFLOW'
ENV_VAR_DEFAULT = 'AIRFLOW_HOME'
AIRFLOW_HOME = os.environ.get('AIRFLOW_HOME', '/opt/airflow')
WEBSERVER_CONFIG = AIRFLOW_HOME + '/webserver_config.py'
DB_CONN_ID = 'postgres_default'

# Determine the current environment (dev, qa, prod)
current_environment = os.environ.get(f'{ENV_VAR_PREFIX}_ENV', 'dev').lower()

# Import environment-specific configuration
try:
    if current_environment == 'prod':
        from config.composer_prod import config
    elif current_environment == 'qa':
        from config.composer_qa import config
    else:  # Default to dev
        from config.composer_dev import config
    
    print(f"Loaded configuration for {current_environment} environment")
except ImportError as e:
    print(f"Failed to import configuration for {current_environment} environment: {e}")
    # Provide empty default config to avoid errors
    config = {}

# Extract configuration sections
env_config = config.get('environment', {})
gcp_config = config.get('gcp', {})
airflow_settings = config.get('airflow', {})
composer_config = config.get('composer', {})
storage_config = config.get('storage', {})
database_config = config.get('database', {})
network_config = config.get('network', {})
security_config = config.get('security', {})

# Create ConfigParser for the airflow.cfg file
parser = configparser.ConfigParser()

# [core] section configuration
parallelism = airflow_settings.get('parallelism', 32)
dag_concurrency = airflow_settings.get('dag_concurrency', 16)
max_active_runs_per_dag = airflow_settings.get('max_active_runs_per_dag', 16)
dagbag_import_timeout = airflow_settings.get('dagbag_import_timeout', 60)

parser['core'] = {
    'dags_folder': os.path.join(AIRFLOW_HOME, 'dags'),
    'executor': airflow_settings.get('executor', 'CeleryExecutor'),
    'sql_alchemy_conn': os.environ.get(
        f'{ENV_VAR_PREFIX}__CORE__SQL_ALCHEMY_CONN', 
        f'postgresql+psycopg2://airflow:airflow@postgres/airflow'
    ),
    'sql_engine_encoding': 'utf-8',
    'load_examples': str(airflow_settings.get('load_examples', False)).lower(),
    'fernet_key': os.environ.get(f'{ENV_VAR_PREFIX}__CORE__FERNET_KEY', ''),
    'parallelism': str(parallelism),
    'dag_concurrency': str(dag_concurrency),
    'max_active_runs_per_dag': str(max_active_runs_per_dag),
    'dagbag_import_timeout': str(dagbag_import_timeout),
    'default_timezone': airflow_settings.get('default_timezone', 'UTC'),
    'security': 'true',
    'plugins_folder': os.path.join(AIRFLOW_HOME, 'plugins'),
}

# [database] section configuration
# Set pool sizes based on environment
sql_alchemy_pool_size = 5
sql_alchemy_max_overflow = 10
if current_environment == 'qa':
    sql_alchemy_pool_size = 10
    sql_alchemy_max_overflow = 20
elif current_environment == 'prod':
    sql_alchemy_pool_size = 20
    sql_alchemy_max_overflow = 40

parser['database'] = {
    'sql_alchemy_pool_size': str(sql_alchemy_pool_size),
    'sql_alchemy_max_overflow': str(sql_alchemy_max_overflow),
    'sql_alchemy_pool_timeout': '30',
    'sql_alchemy_pool_recycle': '1800',
    'sql_alchemy_schema': '',
}

# [logging] section configuration
logs_bucket = storage_config.get('logs_bucket', f'composer2-migration-{current_environment}-logs')
logging_level = composer_config.get('log_level', 'INFO')

parser['logging'] = {
    'base_log_folder': os.path.join(AIRFLOW_HOME, 'logs'),
    'remote_logging': 'true',
    'remote_log_conn_id': 'google_cloud_default',
    'remote_base_log_folder': f'gs://{logs_bucket}/airflow-logs/',
    'logging_level': logging_level,
    'logging_config_class': '',
    'log_processor_timeout': '5',
}

# [webserver] section configuration
# Set number of workers based on environment
workers = 4
if current_environment == 'qa':
    workers = 8
elif current_environment == 'prod':
    workers = 16

# Set navbar color based on environment for visual differentiation
navbar_color = '#fff'
if current_environment == 'dev':
    navbar_color = '#e6ffe6'  # Light green for dev
elif current_environment == 'qa':
    navbar_color = '#e6e6ff'  # Light blue for QA
elif current_environment == 'prod':
    navbar_color = '#ffe6e6'  # Light red for prod

parser['webserver'] = {
    'base_url': '',
    'web_server_host': '0.0.0.0',
    'web_server_port': '8080',
    'web_server_ssl_cert': '',
    'web_server_ssl_key': '',
    'web_server_worker_timeout': '120',
    'worker_refresh_batch_size': '1',
    'worker_refresh_interval': '30',
    'secret_key': os.environ.get(f'{ENV_VAR_PREFIX}__WEBSERVER__SECRET_KEY', 'temporary_key'),
    'workers': str(workers),
    'worker_class': 'gunicorn.workers.gthread.ThreadWorker',
    'access_logfile': os.path.join(AIRFLOW_HOME, 'logs', 'gunicorn_access.log'),
    'error_logfile': os.path.join(AIRFLOW_HOME, 'logs', 'gunicorn_error.log'),
    'expose_config': 'false',
    'authenticate': 'true',
    'auth_backend': 'airflow.providers.google.common.auth_backend.google_openid',
    'dag_default_view': 'grid',
    'dag_orientation': 'TB',
    'rbac': 'true',
    'filter_by_owner': 'false',
    'navbar_color': navbar_color,
}

# [scheduler] section configuration
# Set parsing processes based on environment
parsing_processes = 4
if current_environment == 'qa':
    parsing_processes = 8
elif current_environment == 'prod':
    parsing_processes = 12

# Set StatsD host based on environment
statsd_host = 'localhost'
if current_environment == 'qa':
    statsd_host = 'metrics-qa.example.com'
elif current_environment == 'prod':
    statsd_host = 'metrics-prod.example.com'

parser['scheduler'] = {
    'child_process_log_directory': os.path.join(AIRFLOW_HOME, 'logs', 'scheduler'),
    'min_file_process_interval': '30',
    'dag_dir_list_interval': '60',
    'print_stats_interval': '30',
    'scheduler_heartbeat_sec': '5',
    'scheduler_health_check_threshold': '30',
    'parsing_processes': str(parsing_processes),
    'statsd_on': 'true',
    'statsd_host': statsd_host,
    'statsd_port': '8125',
    'statsd_prefix': 'airflow',
    'run_duration': '-1',
    'job_heartbeat_sec': '5',
}

# [celery] section configuration
# Set worker concurrency based on environment
worker_concurrency = 16
if current_environment == 'qa':
    worker_concurrency = 32
elif current_environment == 'prod':
    worker_concurrency = 64

# Get Redis connection details from environment variables or use defaults
redis_password = os.environ.get(f'{ENV_VAR_PREFIX}__CELERY__REDIS_PASSWORD', '')
redis_host = os.environ.get(f'{ENV_VAR_PREFIX}__CELERY__REDIS_HOST', 'redis')
redis_port = os.environ.get(f'{ENV_VAR_PREFIX}__CELERY__REDIS_PORT', '6379')
redis_db = os.environ.get(f'{ENV_VAR_PREFIX}__CELERY__REDIS_DB', '0')

broker_url = f'redis://:{redis_password}@{redis_host}:{redis_port}/{redis_db}'
result_backend = os.environ.get(
    f'{ENV_VAR_PREFIX}__CELERY__RESULT_BACKEND',
    f'db+postgresql://airflow:airflow@postgres/airflow'
)

parser['celery'] = {
    'broker_url': broker_url,
    'result_backend': result_backend,
    'worker_log_server_port': '8793',
    'worker_concurrency': str(worker_concurrency),
    'worker_prefetch_multiplier': '1',
    'task_acks_late': 'true',
    'task_default_queue': 'default',
    'task_acks_on_failure_or_timeout': 'true',
}

# [api] section configuration
# Strict security for production, more permissive for dev/qa
api_auth_backend = 'airflow.api.auth.backend.deny_all'
if current_environment != 'prod':
    api_auth_backend = 'airflow.api.auth.backend.basic_auth'

parser['api'] = {
    'auth_backend': api_auth_backend,
    'enable_experimental_api': 'false',
}

# [email] section configuration
# Set email configuration with environment-specific sender address
smtp_host = os.environ.get(f'{ENV_VAR_PREFIX}__EMAIL__SMTP_HOST', 'smtp.gmail.com')
smtp_mail_from = airflow_settings.get('default_email_recipient', f'airflow-alerts-{current_environment}@example.com')

parser['email'] = {
    'email_backend': 'airflow.utils.email.send_email_smtp',
    'smtp_host': smtp_host,
    'smtp_starttls': 'true',
    'smtp_ssl': 'false',
    'smtp_port': '587',
    'smtp_mail_from': smtp_mail_from,
    'smtp_timeout': '30',
    'smtp_retry_limit': '5',
}

# [secrets] section configuration
# Configure Google Cloud Secret Manager integration
project_id = gcp_config.get('project_id', '')
secret_prefix = security_config.get('secret_manager_prefix', f'airflow-{current_environment}')

backend_kwargs = {
    'connections_prefix': f'{secret_prefix}-connections',
    'variables_prefix': f'{secret_prefix}-variables',
    'gcp_key_path': '',
    'project_id': project_id
}

parser['secrets'] = {
    'backend': 'airflow.providers.google.cloud.secrets.secret_manager.CloudSecretManagerBackend',
    'backend_kwargs': str(backend_kwargs),
}

# [metrics] section configuration
# Use the same StatsD host as in the scheduler section
parser['metrics'] = {
    'statsd_on': 'true',
    'statsd_host': statsd_host,
    'statsd_port': '8125',
    'statsd_prefix': 'airflow',
}

# Write the configuration to the airflow.cfg file
config_file_path = os.path.join(AIRFLOW_HOME, 'airflow.cfg')
with open(config_file_path, 'w') as config_file:
    parser.write(config_file)

if __name__ == "__main__":
    print(f"Generated Airflow configuration at {config_file_path}")
    hostname = socket.gethostname()
    print(f"Configuration generated for {hostname} in {current_environment} environment")